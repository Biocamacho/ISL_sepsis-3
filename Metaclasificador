#Uso de los cojuntos entrenamiento y test
x_train, x_test, y_train, y_test = get_train_test(2,4)
#Definición de los hiperparámetros usados para el XGBoost
xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=1, eta=0.1, gamma=0,
              learning_rate=0.1, max_delta_step=0, max_depth=18,
              min_child_weight=2, missing=None, n_estimators=400, n_jobs=1,# se cambia el n_estimator a 400
              nthread=None, objective='binary:logistic', random_state=42,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
              silent=None, subsample=1, verbosity=1)
              
              knn = custom_KNN(k=13)#cambio por 9, original 17
knn.fit(x_train[[0,1,2,3,4,5,6]], y_train) #<--cambio se agrega 7
xgb.fit(x_train[columns], y_train)

y_pred_knn = np.transpose(knn.predict_proba(x_test[[0,1,2,3,4,5,6]]))[1]#<--cambio se agrega 7
y_pred_xgb = np.transpose(xgb.predict_proba(x_test[columns]))[1]


y_pred_train_xgb = np.transpose(xgb.predict_proba(x_train[columns]))[1]
y_pred_train_knn = np.transpose(knn.predict_proba(x_train[[0,1,2,3,4,5,6]]))[1]#<--cambio se agrega 7
mid_train = pd.DataFrame(np.transpose([y_pred_train_xgb, y_pred_train_knn]))
mid_test = pd.DataFrame(np.transpose([y_pred_xgb, y_pred_knn]))

lr = LogisticRegression()
lr.fit(mid_train, y_train)

y_pred = np.transpose(lr.predict_proba(mid_test))[1]
